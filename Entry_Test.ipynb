{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Добываем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-gpu==1.14.0 in ./py2/lib/python3.7/site-packages (1.14.0)\n",
      "Requirement already satisfied: keras in ./py2/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: keras-bert in ./py2/lib/python3.7/site-packages (0.80.0)\n",
      "Requirement already satisfied: keras-rectified-adam in ./py2/lib/python3.7/site-packages (0.17.0)\n",
      "Requirement already satisfied: bert-tensorflow in ./py2/lib/python3.7/site-packages (1.0.1)\n",
      "Requirement already satisfied: tqdm in ./py2/lib/python3.7/site-packages (4.36.1)\n",
      "Requirement already satisfied: wget in ./py2/lib/python3.7/site-packages (3.2)\n",
      "Requirement already satisfied: jupyter in ./py2/lib/python3.7/site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy in ./py2/lib/python3.7/site-packages (1.17.3)\n",
      "Requirement already satisfied: pandas in ./py2/lib/python3.7/site-packages (0.25.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in ./py2/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (1.0.8)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in ./py2/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (3.10.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in ./py2/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (0.8.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in ./py2/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (0.1.7)\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in ./py2/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (1.14.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in ./py2/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (0.3.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./py2/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in ./py2/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (0.8.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in ./py2/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (1.24.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in ./py2/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in ./py2/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (1.11.2)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in ./py2/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (1.14.0)\n",
      "Requirement already satisfied: wheel>=0.26 in ./py2/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (0.33.6)\n",
      "Requirement already satisfied: six>=1.10.0 in ./py2/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (1.12.0)\n",
      "Requirement already satisfied: h5py in ./py2/lib/python3.7/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in ./py2/lib/python3.7/site-packages (from keras) (5.1.2)\n",
      "Requirement already satisfied: scipy>=0.14 in ./py2/lib/python3.7/site-packages (from keras) (1.3.1)\n",
      "Requirement already satisfied: keras-transformer>=0.30.0 in ./py2/lib/python3.7/site-packages (from keras-bert) (0.31.0)\n",
      "Requirement already satisfied: ipykernel in ./py2/lib/python3.7/site-packages (from jupyter) (5.1.3)\n",
      "Requirement already satisfied: jupyter-console in ./py2/lib/python3.7/site-packages (from jupyter) (6.0.0)\n",
      "Requirement already satisfied: nbconvert in ./py2/lib/python3.7/site-packages (from jupyter) (5.6.1)\n",
      "Requirement already satisfied: qtconsole in ./py2/lib/python3.7/site-packages (from jupyter) (4.5.5)\n",
      "Requirement already satisfied: ipywidgets in ./py2/lib/python3.7/site-packages (from jupyter) (7.5.1)\n",
      "Requirement already satisfied: notebook in ./py2/lib/python3.7/site-packages (from jupyter) (6.0.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in ./py2/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in ./py2/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: setuptools in ./py2/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow-gpu==1.14.0) (41.4.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in ./py2/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (0.16.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./py2/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.1.1)\n",
      "Requirement already satisfied: keras-pos-embd>=0.10.0 in ./py2/lib/python3.7/site-packages (from keras-transformer>=0.30.0->keras-bert) (0.11.0)\n",
      "Requirement already satisfied: keras-layer-normalization>=0.12.0 in ./py2/lib/python3.7/site-packages (from keras-transformer>=0.30.0->keras-bert) (0.13.0)\n",
      "Requirement already satisfied: keras-embed-sim>=0.7.0 in ./py2/lib/python3.7/site-packages (from keras-transformer>=0.30.0->keras-bert) (0.7.0)\n",
      "Requirement already satisfied: keras-position-wise-feed-forward>=0.5.0 in ./py2/lib/python3.7/site-packages (from keras-transformer>=0.30.0->keras-bert) (0.6.0)\n",
      "Requirement already satisfied: keras-multi-head>=0.22.0 in ./py2/lib/python3.7/site-packages (from keras-transformer>=0.30.0->keras-bert) (0.22.0)\n",
      "Requirement already satisfied: jupyter-client in ./py2/lib/python3.7/site-packages (from ipykernel->jupyter) (5.3.4)\n",
      "Requirement already satisfied: ipython>=5.0.0 in ./py2/lib/python3.7/site-packages (from ipykernel->jupyter) (7.9.0)\n",
      "Requirement already satisfied: traitlets>=4.1.0 in ./py2/lib/python3.7/site-packages (from ipykernel->jupyter) (4.3.3)\n",
      "Requirement already satisfied: tornado>=4.2 in ./py2/lib/python3.7/site-packages (from ipykernel->jupyter) (6.0.3)\n",
      "Requirement already satisfied: pygments in ./py2/lib/python3.7/site-packages (from jupyter-console->jupyter) (2.4.2)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in ./py2/lib/python3.7/site-packages (from jupyter-console->jupyter) (2.0.10)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in ./py2/lib/python3.7/site-packages (from nbconvert->jupyter) (0.3)\n",
      "Requirement already satisfied: defusedxml in ./py2/lib/python3.7/site-packages (from nbconvert->jupyter) (0.6.0)\n",
      "Requirement already satisfied: jinja2>=2.4 in ./py2/lib/python3.7/site-packages (from nbconvert->jupyter) (2.10.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in ./py2/lib/python3.7/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: nbformat>=4.4 in ./py2/lib/python3.7/site-packages (from nbconvert->jupyter) (4.4.0)\n",
      "Requirement already satisfied: testpath in ./py2/lib/python3.7/site-packages (from nbconvert->jupyter) (0.4.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in ./py2/lib/python3.7/site-packages (from nbconvert->jupyter) (1.4.2)\n",
      "Requirement already satisfied: jupyter-core in ./py2/lib/python3.7/site-packages (from nbconvert->jupyter) (4.6.1)\n",
      "Requirement already satisfied: bleach in ./py2/lib/python3.7/site-packages (from nbconvert->jupyter) (3.1.0)\n",
      "Requirement already satisfied: ipython-genutils in ./py2/lib/python3.7/site-packages (from qtconsole->jupyter) (0.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in ./py2/lib/python3.7/site-packages (from ipywidgets->jupyter) (3.5.1)\n",
      "Requirement already satisfied: terminado>=0.8.1 in ./py2/lib/python3.7/site-packages (from notebook->jupyter) (0.8.2)\n",
      "Requirement already satisfied: Send2Trash in ./py2/lib/python3.7/site-packages (from notebook->jupyter) (1.5.0)\n",
      "Requirement already satisfied: prometheus-client in ./py2/lib/python3.7/site-packages (from notebook->jupyter) (0.7.1)\n",
      "Requirement already satisfied: pyzmq>=17 in ./py2/lib/python3.7/site-packages (from notebook->jupyter) (18.1.0)\n",
      "Requirement already satisfied: keras-self-attention==0.41.0 in ./py2/lib/python3.7/site-packages (from keras-multi-head>=0.22.0->keras-transformer>=0.30.0->keras-bert) (0.41.0)\n",
      "Requirement already satisfied: jedi>=0.10 in ./py2/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->jupyter) (0.15.1)\n",
      "Requirement already satisfied: pickleshare in ./py2/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->jupyter) (0.7.5)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in ./py2/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->jupyter) (4.7.0)\n",
      "Requirement already satisfied: decorator in ./py2/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->jupyter) (4.4.1)\n",
      "Requirement already satisfied: backcall in ./py2/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->jupyter) (0.1.0)\n",
      "Requirement already satisfied: wcwidth in ./py2/lib/python3.7/site-packages (from prompt-toolkit<2.1.0,>=2.0.0->jupyter-console->jupyter) (0.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in ./py2/lib/python3.7/site-packages (from jinja2>=2.4->nbconvert->jupyter) (1.1.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in ./py2/lib/python3.7/site-packages (from nbformat>=4.4->nbconvert->jupyter) (3.1.1)\n",
      "Requirement already satisfied: webencodings in ./py2/lib/python3.7/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: ptyprocess; os_name != \"nt\" in ./py2/lib/python3.7/site-packages (from terminado>=0.8.1->notebook->jupyter) (0.6.0)\n",
      "Requirement already satisfied: parso>=0.5.0 in ./py2/lib/python3.7/site-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->jupyter) (0.5.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in ./py2/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (19.3.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyrsistent>=0.14.0 in ./py2/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.15.5)\r\n",
      "Requirement already satisfied: importlib-metadata in ./py2/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.23)\r\n",
      "Requirement already satisfied: zipp>=0.5 in ./py2/lib/python3.7/site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.6.0)\r\n",
      "Requirement already satisfied: more-itertools in ./py2/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (7.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow-gpu==1.14.0 keras keras-bert keras-rectified-adam bert-tensorflow tqdm wget jupyter numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from keras_radam import RAdam\n",
    "from keras.models import Model, load_model\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=PendingDeprecationWarning)\n",
    "from keras_bert import load_trained_model_from_checkpoint, get_custom_objects, Tokenizer\n",
    "import codecs\n",
    "from bert.tokenization import FullTokenizer\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "SEQ_LEN = 512\n",
    "LR = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_creator_url = 'https://github.com/rkadlec/ubuntu-ranking-dataset-creator/archive/master.zip'\n",
    "creator_filename = wget.download(dataset_creator_url)\n",
    "with ZipFile(creator_filename, 'r') as zipfile:\n",
    "    zipfile.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('./ubuntu-ranking-dataset-creator-master/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "Requirement already up-to-date: pip in /home/lexi/.local/lib/python2.7/site-packages (19.3.1)\n",
      "Requirement already up-to-date: six>=1.10.0 in /home/lexi/.local/lib/python2.7/site-packages (from -r ../requirements.txt (line 1)) (1.12.0)\n",
      "Requirement already up-to-date: unicodecsv>=0.14.0 in /home/lexi/.local/lib/python2.7/site-packages (from -r ../requirements.txt (line 2)) (0.14.1)\n",
      "Requirement already up-to-date: nltk>=3.1 in /home/lexi/.local/lib/python2.7/site-packages (from -r ../requirements.txt (line 3)) (3.4.5)\n",
      "Requirement already satisfied, skipping upgrade: singledispatch; python_version < \"3.4\" in /home/lexi/.local/lib/python2.7/site-packages (from nltk>=3.1->-r ../requirements.txt (line 3)) (3.4.0.3)\n"
     ]
    }
   ],
   "source": [
    "! chmod 777 generate.sh\n",
    "! pip2 install --user --upgrade pip -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lexi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/lexi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Downloading http://cs.mcgill.ca/~jpineau/datasets/ubuntu-corpus-1.0/ubuntu_dialogs.tgz to ./ubuntu_dialogs.tgz\n",
      "Successfully downloaded ./ubuntu_dialogs.tgz\n",
      "Unpacking dialogs ...\n",
      "Archive unpacked.\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "155000\n",
      "156000\n",
      "157000\n",
      "158000\n",
      "159000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "164000\n",
      "165000\n",
      "166000\n",
      "167000\n",
      "168000\n",
      "169000\n",
      "170000\n",
      "171000\n",
      "172000\n",
      "173000\n",
      "174000\n",
      "175000\n",
      "176000\n",
      "177000\n",
      "178000\n",
      "179000\n",
      "180000\n",
      "181000\n",
      "182000\n",
      "183000\n",
      "184000\n",
      "185000\n",
      "186000\n",
      "187000\n",
      "188000\n",
      "189000\n",
      "190000\n",
      "191000\n",
      "192000\n",
      "193000\n",
      "194000\n",
      "195000\n",
      "196000\n",
      "197000\n",
      "198000\n",
      "199000\n",
      "200000\n",
      "201000\n",
      "202000\n",
      "203000\n",
      "204000\n",
      "205000\n",
      "206000\n",
      "207000\n",
      "208000\n",
      "209000\n",
      "210000\n",
      "211000\n",
      "212000\n",
      "213000\n",
      "214000\n",
      "215000\n",
      "216000\n",
      "217000\n",
      "218000\n",
      "219000\n",
      "220000\n",
      "221000\n",
      "222000\n",
      "223000\n",
      "224000\n",
      "225000\n",
      "226000\n",
      "227000\n",
      "228000\n",
      "229000\n",
      "230000\n",
      "231000\n",
      "232000\n",
      "233000\n",
      "234000\n",
      "235000\n",
      "236000\n",
      "237000\n",
      "238000\n",
      "239000\n",
      "240000\n",
      "241000\n",
      "242000\n",
      "243000\n",
      "244000\n",
      "245000\n",
      "246000\n",
      "247000\n",
      "248000\n",
      "249000\n",
      "250000\n",
      "251000\n",
      "252000\n",
      "253000\n",
      "254000\n",
      "255000\n",
      "256000\n",
      "257000\n",
      "258000\n",
      "259000\n",
      "260000\n",
      "261000\n",
      "262000\n",
      "263000\n",
      "264000\n",
      "265000\n",
      "266000\n",
      "267000\n",
      "268000\n",
      "269000\n",
      "270000\n",
      "271000\n",
      "272000\n",
      "273000\n",
      "274000\n",
      "275000\n",
      "276000\n",
      "277000\n",
      "278000\n",
      "279000\n",
      "280000\n",
      "281000\n",
      "282000\n",
      "283000\n",
      "284000\n",
      "285000\n",
      "286000\n",
      "287000\n",
      "288000\n",
      "289000\n",
      "290000\n",
      "291000\n",
      "292000\n",
      "293000\n",
      "294000\n",
      "295000\n",
      "296000\n",
      "297000\n",
      "298000\n",
      "299000\n",
      "300000\n",
      "301000\n",
      "302000\n",
      "303000\n",
      "304000\n",
      "305000\n",
      "306000\n",
      "307000\n",
      "308000\n",
      "309000\n",
      "310000\n",
      "311000\n",
      "312000\n",
      "313000\n",
      "314000\n",
      "315000\n",
      "316000\n",
      "317000\n",
      "318000\n",
      "319000\n",
      "320000\n",
      "321000\n",
      "322000\n",
      "323000\n",
      "324000\n",
      "325000\n",
      "326000\n",
      "327000\n",
      "328000\n",
      "329000\n",
      "330000\n",
      "331000\n",
      "332000\n",
      "333000\n",
      "334000\n",
      "335000\n",
      "336000\n",
      "337000\n",
      "338000\n",
      "339000\n",
      "340000\n",
      "341000\n",
      "342000\n",
      "343000\n",
      "344000\n",
      "345000\n",
      "346000\n",
      "347000\n",
      "348000\n",
      "349000\n",
      "350000\n",
      "351000\n",
      "352000\n",
      "353000\n",
      "354000\n",
      "355000\n",
      "356000\n",
      "357000\n",
      "358000\n",
      "359000\n",
      "360000\n",
      "361000\n",
      "362000\n",
      "363000\n",
      "364000\n",
      "365000\n",
      "366000\n",
      "367000\n",
      "368000\n",
      "369000\n",
      "370000\n",
      "371000\n",
      "372000\n",
      "373000\n",
      "374000\n",
      "375000\n",
      "376000\n",
      "377000\n",
      "378000\n",
      "379000\n",
      "380000\n",
      "381000\n",
      "382000\n",
      "383000\n",
      "384000\n",
      "385000\n",
      "386000\n",
      "387000\n",
      "388000\n",
      "389000\n",
      "390000\n",
      "391000\n",
      "392000\n",
      "393000\n",
      "394000\n",
      "395000\n",
      "396000\n",
      "397000\n",
      "398000\n",
      "399000\n",
      "400000\n",
      "401000\n",
      "402000\n",
      "403000\n",
      "404000\n",
      "405000\n",
      "406000\n",
      "407000\n",
      "408000\n",
      "409000\n",
      "410000\n",
      "411000\n",
      "412000\n",
      "413000\n",
      "414000\n",
      "415000\n",
      "416000\n",
      "417000\n",
      "418000\n",
      "419000\n",
      "420000\n",
      "421000\n",
      "422000\n",
      "423000\n",
      "424000\n",
      "425000\n",
      "426000\n",
      "427000\n",
      "428000\n",
      "429000\n",
      "430000\n",
      "431000\n",
      "432000\n",
      "433000\n",
      "434000\n",
      "435000\n",
      "436000\n",
      "437000\n",
      "438000\n",
      "439000\n",
      "440000\n",
      "441000\n",
      "442000\n",
      "443000\n",
      "444000\n",
      "445000\n",
      "446000\n",
      "447000\n",
      "448000\n",
      "449000\n",
      "450000\n",
      "451000\n",
      "452000\n",
      "453000\n",
      "454000\n",
      "455000\n",
      "456000\n",
      "457000\n",
      "458000\n",
      "459000\n",
      "460000\n",
      "461000\n",
      "462000\n",
      "463000\n",
      "464000\n",
      "465000\n",
      "466000\n",
      "467000\n",
      "468000\n",
      "469000\n",
      "470000\n",
      "471000\n",
      "472000\n",
      "473000\n",
      "474000\n",
      "475000\n",
      "476000\n",
      "477000\n",
      "478000\n",
      "479000\n",
      "480000\n",
      "481000\n",
      "482000\n",
      "483000\n",
      "484000\n",
      "485000\n",
      "486000\n",
      "487000\n",
      "488000\n",
      "489000\n",
      "490000\n",
      "491000\n",
      "492000\n",
      "493000\n",
      "494000\n",
      "495000\n",
      "496000\n",
      "497000\n",
      "498000\n",
      "499000\n",
      "500000\n",
      "501000\n",
      "502000\n",
      "503000\n",
      "504000\n",
      "505000\n",
      "506000\n",
      "507000\n",
      "508000\n",
      "509000\n",
      "510000\n",
      "511000\n",
      "512000\n",
      "513000\n",
      "514000\n",
      "515000\n",
      "516000\n",
      "517000\n",
      "518000\n",
      "519000\n",
      "520000\n",
      "521000\n",
      "522000\n",
      "523000\n",
      "524000\n",
      "525000\n",
      "526000\n",
      "527000\n",
      "528000\n",
      "529000\n",
      "530000\n",
      "531000\n",
      "532000\n",
      "533000\n",
      "534000\n",
      "535000\n",
      "536000\n",
      "537000\n",
      "538000\n",
      "539000\n",
      "540000\n",
      "541000\n",
      "542000\n",
      "543000\n",
      "544000\n",
      "545000\n",
      "546000\n",
      "547000\n",
      "548000\n",
      "549000\n",
      "550000\n",
      "551000\n",
      "552000\n",
      "553000\n",
      "554000\n",
      "555000\n",
      "556000\n",
      "557000\n",
      "558000\n",
      "559000\n",
      "560000\n",
      "561000\n",
      "562000\n",
      "563000\n",
      "564000\n",
      "565000\n",
      "566000\n",
      "567000\n",
      "568000\n",
      "569000\n",
      "570000\n",
      "571000\n",
      "572000\n",
      "573000\n",
      "574000\n",
      "575000\n",
      "576000\n",
      "577000\n",
      "578000\n",
      "579000\n",
      "580000\n",
      "581000\n",
      "582000\n",
      "583000\n",
      "584000\n",
      "585000\n",
      "586000\n",
      "587000\n",
      "588000\n",
      "589000\n",
      "590000\n",
      "591000\n",
      "592000\n",
      "593000\n",
      "594000\n",
      "595000\n",
      "596000\n",
      "597000\n",
      "598000\n",
      "599000\n",
      "600000\n",
      "601000\n",
      "602000\n",
      "603000\n",
      "604000\n",
      "605000\n",
      "606000\n",
      "607000\n",
      "608000\n",
      "609000\n",
      "610000\n",
      "611000\n",
      "612000\n",
      "613000\n",
      "614000\n",
      "615000\n",
      "616000\n",
      "617000\n",
      "618000\n",
      "619000\n",
      "620000\n",
      "621000\n",
      "622000\n",
      "623000\n",
      "624000\n",
      "625000\n",
      "626000\n",
      "627000\n",
      "628000\n",
      "629000\n",
      "630000\n",
      "631000\n",
      "632000\n",
      "633000\n",
      "634000\n",
      "635000\n",
      "636000\n",
      "637000\n",
      "638000\n",
      "639000\n",
      "640000\n",
      "641000\n",
      "642000\n",
      "643000\n",
      "644000\n",
      "645000\n",
      "646000\n",
      "647000\n",
      "648000\n",
      "649000\n",
      "650000\n",
      "651000\n",
      "652000\n",
      "653000\n",
      "654000\n",
      "655000\n",
      "656000\n",
      "657000\n",
      "658000\n",
      "659000\n",
      "660000\n",
      "661000\n",
      "662000\n",
      "663000\n",
      "664000\n",
      "665000\n",
      "666000\n",
      "667000\n",
      "668000\n",
      "669000\n",
      "670000\n",
      "671000\n",
      "672000\n",
      "673000\n",
      "674000\n",
      "675000\n",
      "676000\n",
      "677000\n",
      "678000\n",
      "679000\n",
      "680000\n",
      "681000\n",
      "682000\n",
      "683000\n",
      "684000\n",
      "685000\n",
      "686000\n",
      "687000\n",
      "688000\n",
      "689000\n",
      "690000\n",
      "691000\n",
      "692000\n",
      "693000\n",
      "694000\n",
      "695000\n",
      "696000\n",
      "697000\n",
      "698000\n",
      "699000\n",
      "700000\n",
      "701000\n",
      "702000\n",
      "703000\n",
      "704000\n",
      "705000\n",
      "706000\n",
      "707000\n",
      "708000\n",
      "709000\n",
      "710000\n",
      "711000\n",
      "712000\n",
      "713000\n",
      "714000\n",
      "715000\n",
      "716000\n",
      "717000\n",
      "718000\n",
      "719000\n",
      "720000\n",
      "721000\n",
      "722000\n",
      "723000\n",
      "724000\n",
      "725000\n",
      "726000\n",
      "727000\n",
      "728000\n",
      "729000\n",
      "730000\n",
      "731000\n",
      "732000\n",
      "733000\n",
      "734000\n",
      "735000\n",
      "736000\n",
      "737000\n",
      "738000\n",
      "739000\n",
      "740000\n",
      "741000\n",
      "742000\n",
      "743000\n",
      "744000\n",
      "745000\n",
      "746000\n",
      "747000\n",
      "748000\n",
      "749000\n",
      "750000\n",
      "751000\n",
      "752000\n",
      "753000\n",
      "754000\n",
      "755000\n",
      "756000\n",
      "757000\n",
      "758000\n",
      "759000\n",
      "760000\n",
      "761000\n",
      "762000\n",
      "763000\n",
      "764000\n",
      "765000\n",
      "766000\n",
      "767000\n",
      "768000\n",
      "769000\n",
      "770000\n",
      "771000\n",
      "772000\n",
      "773000\n",
      "774000\n",
      "775000\n",
      "776000\n",
      "777000\n",
      "778000\n",
      "779000\n",
      "780000\n",
      "781000\n",
      "782000\n",
      "783000\n",
      "784000\n",
      "785000\n",
      "786000\n",
      "787000\n",
      "788000\n",
      "789000\n",
      "790000\n",
      "791000\n",
      "792000\n",
      "793000\n",
      "794000\n",
      "795000\n",
      "796000\n",
      "797000\n",
      "798000\n",
      "799000\n",
      "800000\n",
      "801000\n",
      "802000\n",
      "803000\n",
      "804000\n",
      "805000\n",
      "806000\n",
      "807000\n",
      "808000\n",
      "809000\n",
      "810000\n",
      "811000\n",
      "812000\n",
      "813000\n",
      "814000\n",
      "815000\n",
      "816000\n",
      "817000\n",
      "818000\n",
      "819000\n",
      "820000\n",
      "821000\n",
      "822000\n",
      "823000\n",
      "824000\n",
      "825000\n",
      "826000\n",
      "827000\n",
      "828000\n",
      "829000\n",
      "830000\n",
      "831000\n",
      "832000\n",
      "833000\n",
      "834000\n",
      "835000\n",
      "836000\n",
      "837000\n",
      "838000\n",
      "839000\n",
      "840000\n",
      "841000\n",
      "842000\n",
      "843000\n",
      "844000\n",
      "845000\n",
      "846000\n",
      "847000\n",
      "848000\n",
      "849000\n",
      "850000\n",
      "851000\n",
      "852000\n",
      "853000\n",
      "854000\n",
      "855000\n",
      "856000\n",
      "857000\n",
      "858000\n",
      "859000\n",
      "860000\n",
      "861000\n",
      "862000\n",
      "863000\n",
      "864000\n",
      "865000\n",
      "866000\n",
      "867000\n",
      "868000\n",
      "869000\n",
      "870000\n",
      "871000\n",
      "872000\n",
      "873000\n",
      "874000\n",
      "875000\n",
      "876000\n",
      "877000\n",
      "878000\n",
      "879000\n",
      "880000\n",
      "881000\n",
      "882000\n",
      "883000\n",
      "884000\n",
      "885000\n",
      "886000\n",
      "887000\n",
      "888000\n",
      "889000\n",
      "890000\n",
      "891000\n",
      "892000\n",
      "893000\n",
      "894000\n",
      "895000\n",
      "896000\n",
      "897000\n",
      "898000\n",
      "899000\n",
      "900000\n",
      "901000\n",
      "902000\n",
      "903000\n",
      "904000\n",
      "905000\n",
      "906000\n",
      "907000\n",
      "908000\n",
      "909000\n",
      "910000\n",
      "911000\n",
      "912000\n",
      "913000\n",
      "914000\n",
      "915000\n",
      "916000\n",
      "917000\n",
      "918000\n",
      "919000\n",
      "920000\n",
      "921000\n",
      "922000\n",
      "923000\n",
      "924000\n",
      "925000\n",
      "926000\n",
      "927000\n",
      "928000\n",
      "929000\n",
      "930000\n",
      "931000\n",
      "932000\n",
      "933000\n",
      "934000\n",
      "935000\n",
      "936000\n",
      "937000\n",
      "938000\n",
      "939000\n",
      "940000\n",
      "941000\n",
      "942000\n",
      "943000\n",
      "944000\n",
      "945000\n",
      "946000\n",
      "947000\n",
      "948000\n",
      "949000\n",
      "950000\n",
      "951000\n",
      "952000\n",
      "953000\n",
      "954000\n",
      "955000\n",
      "956000\n",
      "957000\n",
      "958000\n",
      "959000\n",
      "960000\n",
      "961000\n",
      "962000\n",
      "963000\n",
      "964000\n",
      "965000\n",
      "966000\n",
      "967000\n",
      "968000\n",
      "969000\n",
      "970000\n",
      "971000\n",
      "972000\n",
      "973000\n",
      "974000\n",
      "975000\n",
      "976000\n",
      "977000\n",
      "978000\n",
      "979000\n",
      "980000\n",
      "981000\n",
      "982000\n",
      "983000\n",
      "984000\n",
      "985000\n",
      "986000\n",
      "987000\n",
      "988000\n",
      "989000\n",
      "990000\n",
      "991000\n",
      "992000\n",
      "993000\n",
      "994000\n",
      "995000\n",
      "996000\n",
      "997000\n",
      "998000\n",
      "999000\n",
      "Train dataset stored in: train.csv\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "Dataset stored in: test.csv\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "Dataset stored in: valid.csv\n"
     ]
    }
   ],
   "source": [
    "# change python commands in generate.sh to python2\n",
    "! ./generate.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "val_data = pd.read_csv('valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['Context', 'Utterance', 'Label'], dtype='object'),\n",
       " Index(['Context', 'Ground Truth Utterance', 'Distractor_0', 'Distractor_1',\n",
       "        'Distractor_2', 'Distractor_3', 'Distractor_4', 'Distractor_5',\n",
       "        'Distractor_6', 'Distractor_7', 'Distractor_8'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns, test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>'booby raid' is __eou__ hehe ..  im jsut tring to get silicon raid image a /dev/something so i can cp my filesystem to it and edit fstab __eou__ mod kernel get /dev/something... then it should be all set. __eou__ damm.. so no one has done anything with raid as the primary file system in here? __eou__ __eot__ I installed onto a software raid1 array once __eou__ __eot__ how u manage that i tried a software raid1 it wouldnt work during install. __eou__ __eot__</td>\n",
       "      <td>I think I made a plain /boot and then put a partition on each disk to be the raid'd / __eou__</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>does gcc come installed by default on ubuntu5.10? __eou__ __eot__ no &gt;&gt; sudo apt-get install build-essential &lt;&lt;will sort that out for you __eou__ __eot__ dailup, was hopeing it at least had the libs __eou__ __eot__ it is on the CD afik __eou__ it is on the CD I just checked __eou__ __eot__</td>\n",
       "      <td>how do I tweak the xorg.conf fire to use GLX and OpenGL __eou__</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>anyone here have kubuntu-desktop installed __eou__ __eot__ I tried it, but it sucked, so I removed it again.  #kubuntu might be a good place to ask, too __eou__ __eot__ they're biased __eou__ __eot__</td>\n",
       "      <td>so are we :) __eou__ kubuntu isn't as complete as ubuntu, in my experience.  they don't have anything to tell them when to update their system, for instance __eou__</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>well __eou__ i wish someone would talk about my system recovery actually... __eou__ anybody has any idea about what it means when my file system is mounted read-only after a system crash... __eou__ __eot__ have you tried just rebooting it again after that? __eou__ __eot__ : but anyway my options are rather limited since I cant actually edit anything being in read-only mode... __eou__ __eot__ true. You could try to remount. Typically, it just means that it's fixed it, but wants you to reboot anyway. __eou__ __eot__</td>\n",
       "      <td>sure not. I remember a live windows but I can't remember the name. SAM part? __eou__</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>i'm having a problem with hedgehog, I am trying to use dialup. The modem initalises, and dials the number... but firefox won't connect. I can't even ping out. Any ideas? __eou__ still connected __eou__ __eot__ Perhaps you need to set ppp0 as default gateway from System-&gt;Administration-&gt;Networking __eou__ __eot__ it is already set as default __eou__ __eot__ Is it only problem with FF? __eou__ __eot__ no, i cannot even ping out. it's like there's no connection __eou__ __eot__</td>\n",
       "      <td>this thread may help: http://ubuntuforums.org/showthread.php?t=1740368 __eou__</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Context  \\\n",
       "999995  'booby raid' is __eou__ hehe ..  im jsut tring to get silicon raid image a /dev/something so i can cp my filesystem to it and edit fstab __eou__ mod kernel get /dev/something... then it should be all set. __eou__ damm.. so no one has done anything with raid as the primary file system in here? __eou__ __eot__ I installed onto a software raid1 array once __eou__ __eot__ how u manage that i tried a software raid1 it wouldnt work during install. __eou__ __eot__                                                              \n",
       "999996  does gcc come installed by default on ubuntu5.10? __eou__ __eot__ no >> sudo apt-get install build-essential <<will sort that out for you __eou__ __eot__ dailup, was hopeing it at least had the libs __eou__ __eot__ it is on the CD afik __eou__ it is on the CD I just checked __eou__ __eot__                                                                                                                                                                                                                                         \n",
       "999997  anyone here have kubuntu-desktop installed __eou__ __eot__ I tried it, but it sucked, so I removed it again.  #kubuntu might be a good place to ask, too __eou__ __eot__ they're biased __eou__ __eot__                                                                                                                                                                                                                                                                                                                                    \n",
       "999998  well __eou__ i wish someone would talk about my system recovery actually... __eou__ anybody has any idea about what it means when my file system is mounted read-only after a system crash... __eou__ __eot__ have you tried just rebooting it again after that? __eou__ __eot__ : but anyway my options are rather limited since I cant actually edit anything being in read-only mode... __eou__ __eot__ true. You could try to remount. Typically, it just means that it's fixed it, but wants you to reboot anyway. __eou__ __eot__    \n",
       "999999  i'm having a problem with hedgehog, I am trying to use dialup. The modem initalises, and dials the number... but firefox won't connect. I can't even ping out. Any ideas? __eou__ still connected __eou__ __eot__ Perhaps you need to set ppp0 as default gateway from System->Administration->Networking __eou__ __eot__ it is already set as default __eou__ __eot__ Is it only problem with FF? __eou__ __eot__ no, i cannot even ping out. it's like there's no connection __eou__ __eot__                                             \n",
       "\n",
       "                                                                                                                                                                   Utterance  \\\n",
       "999995  I think I made a plain /boot and then put a partition on each disk to be the raid'd / __eou__                                                                          \n",
       "999996  how do I tweak the xorg.conf fire to use GLX and OpenGL __eou__                                                                                                        \n",
       "999997  so are we :) __eou__ kubuntu isn't as complete as ubuntu, in my experience.  they don't have anything to tell them when to update their system, for instance __eou__   \n",
       "999998  sure not. I remember a live windows but I can't remember the name. SAM part? __eou__                                                                                   \n",
       "999999  this thread may help: http://ubuntuforums.org/showthread.php?t=1740368 __eou__                                                                                         \n",
       "\n",
       "        Label  \n",
       "999995  1.0    \n",
       "999996  0.0    \n",
       "999997  1.0    \n",
       "999998  0.0    \n",
       "999999  0.0    "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Ground Truth Utterance</th>\n",
       "      <th>Distractor_0</th>\n",
       "      <th>Distractor_1</th>\n",
       "      <th>Distractor_2</th>\n",
       "      <th>Distractor_3</th>\n",
       "      <th>Distractor_4</th>\n",
       "      <th>Distractor_5</th>\n",
       "      <th>Distractor_6</th>\n",
       "      <th>Distractor_7</th>\n",
       "      <th>Distractor_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anyone knows why my stock oneiric exports env var 'USERNAME'?  I mean what is that used for?  I know of $USER but not $USERNAME .  My precise install doesn't export USERNAME __eou__ __eot__ looks like it used to be exported by lightdm, but the line had the comment \"// FIXME: Is this required?\" so I guess it isn't surprising it is gone __eou__ __eot__ thanks!  How the heck did you figure that out? __eou__ __eot__ https://bugs.launchpad.net/lightdm/+bug/864109/comments/3 __eou__ __eot__</td>\n",
       "      <td>nice thanks! __eou__</td>\n",
       "      <td>wrong channel for it, but check efnet.org, unofficial page. __eou__</td>\n",
       "      <td>every time the kernel changes, you will lose video __eou__ yep __eou__</td>\n",
       "      <td>ok __eou__</td>\n",
       "      <td>!nomodeset &gt; acer __eou__ I'm assuming it is a driver issue. __eou__ !pm &gt; acer __eou__ i DON'T pm. ;) __eou__ OOPS SORRY FOR THE CAPS __eou__</td>\n",
       "      <td>http://www.ubuntu.com/project/about-ubuntu/derivatives  (some call them derivatives, others call them flavors, same difference) __eou__</td>\n",
       "      <td>thx __eou__ unfortunately the program isn't installed from the repositories __eou__</td>\n",
       "      <td>how can I check? By doing a recovery for testing? __eou__</td>\n",
       "      <td>my humble apologies __eou__</td>\n",
       "      <td>#ubuntu-offtopic __eou__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i set up my hd such that i have to type a passphrase to access it at boot. how can i remove that passwrd, and just boot up normal. i did this at install, it works fine, just tired of having reboots where i need to be at terminal to type passwd in. help? __eou__ __eot__ backup your data, and re-install without encryption \"might\" be the easiest method __eou__ __eot__</td>\n",
       "      <td>so you dont know, ok, anyone else? __eou__ you are like, yah my mouse doesnt work, reinstall your os lolol what a joke __eou__</td>\n",
       "      <td>nmap is nice, but it wasn't what I was looking for.  I finally found it again: mtr (my traceroute) is what I was looking for.  I'll be keeping nmap handy though. __eou__</td>\n",
       "      <td>ok __eou__</td>\n",
       "      <td>cdrom worked fine on windows. __eou__ i dont think it has anything to do with the buring process, cds work fine on my desktop and my other ubuntu lap __eou__</td>\n",
       "      <td>ah yes, i have read return as rerun __eou__</td>\n",
       "      <td>hm? __eou__</td>\n",
       "      <td>not the case, LTS is every other .04 release. The .04 isn't always more stable __eou__ I would reinstall with Precise __eou__ you can restore user data and such from backup __eou__</td>\n",
       "      <td>Pretty much __eou__</td>\n",
       "      <td>I used the one I downloaded from AMD __eou__</td>\n",
       "      <td>ffmpeg is part of the package , quixotedon , at least I'm quite sure it still is __eou__ if not just install ffmpeg __eou__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im trying to use ubuntu on my macbook pro retina __eou__ i read in the forums that ubuntu has a apple version now? __eou__ __eot__  not that ive ever heard of..  normal ubutnu should work on an intel based mac. there is the PPC version also. __eou__  you want total control? or what are you wanting exactly? __eou__ __eot__</td>\n",
       "      <td>just wondering how it runs __eou__</td>\n",
       "      <td>yes, that's what I did, exported it to a \"id_dsa\" file, then back to Ubuntu copied it into ~/.ssh/ __eou__</td>\n",
       "      <td>nothing - i am talking about the question of myhero __eou__</td>\n",
       "      <td>that should fix the fonts being too large __eou__</td>\n",
       "      <td>okay, so hcitool echos back hci0 &lt;mac address of controller&gt; but the bluetooth devices panel keeps disconnecting and reconnecting the device (or so it seems) any idea why that would be? __eou__</td>\n",
       "      <td>I get to the menu with options such as 'try ubuntu', 'install ubuntu', 'check disc' __eou__</td>\n",
       "      <td>why do u need analyzer __eou__ it is a toy __eou__ ok msp301 __eou__ but y, i mean it is the same ubunut, only with older programs __eou__ ubuntu 804 or 1204 __eou__ no i dont use 804 __eou__ i am asking hypo qs __eou__</td>\n",
       "      <td>Cntrl-C may stop the command but it doesn't fix my HDD problem. __eou__</td>\n",
       "      <td>if you're only going to run Ubuntu, just get a normal PC rather than a mac __eou__ that said, I'm running it on a macbook, because I got one relatively cheaply __eou__</td>\n",
       "      <td>the ones which are not picked up at the moment are on STDERR and not STDOUT and &gt; is only covering STDOUT __eou__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no suggestions? __eou__ links? __eou__ how can i remove luks passphrase at boot. i dont want to use feature anymore... __eou__ __eot__ you may need to create a new volume __eou__ __eot__ that leads me to the next question lol... i dont know how to create new volumes exactly in cmdline, usually i use a gui. im just trying to access this server via usb loaded with next os im going to load, the luks pw is stopping me __eou__ __eot__ for something like that I would likely use something like a live gparted disk to avoid the conflict of editing from the disk __eou__ __eot__</td>\n",
       "      <td>you cant load anything via usb or cd when luks is running __eou__ it wont allow usb boot, i tried with 2 diff usb drives __eou__</td>\n",
       "      <td>-p  sorry... __eou__  nmap -p22 __eou__ It doesn't say:  22/tcp open  ssh  ? __eou__</td>\n",
       "      <td>i guess so i can't even launch it. __eou__</td>\n",
       "      <td>noted __eou__</td>\n",
       "      <td>rxvt-unicode is one __eou__</td>\n",
       "      <td>I tarred all of ~ __eou__</td>\n",
       "      <td>I tarred all of ~ __eou__</td>\n",
       "      <td>I don't really know if I can help, but I was curious. lol __eou__ That's cool. I'll look into it. Now, we better stop talking about this since it's offtopic. :P __eou__</td>\n",
       "      <td>that works just fine, thanks! __eou__</td>\n",
       "      <td>thank you __eou__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I just added a second usb printer but not sure what the uri should read - can anyone help with usb printers? __eou__ __eot__ firefox localhost:631 __eou__ __eot__ firefox? __eou__ __eot__ yes __eou__ firefox localhost:631 __eou__ firefox http://localhost:631 __eou__ cups has a web based interface __eou__ __eot__</td>\n",
       "      <td>i was setting it up under the printer configuration __eou__ thanks! __eou__</td>\n",
       "      <td>i'd say the most commonly venue would be via Launchpad. check out the factoid !bug as well __eou__</td>\n",
       "      <td>the old hardy man page, http://manpages.ubuntu.com/manpages/hardy/man1/gcalctool.1.html says \"delete\" clears the screen, but it doesn't __eou__ because LTS are good __eou__</td>\n",
       "      <td>i'll give a try __eou__</td>\n",
       "      <td>by the way, the url you posted for davfs is from dapper... that's 5.xx iirc __eou__</td>\n",
       "      <td>http://ubuntuforums.org/showthread.php?t=1549847 __eou__</td>\n",
       "      <td>So I load up putty gui, then what do I do? __eou__</td>\n",
       "      <td>you should read error messages, it says 'are you root?' __eou__</td>\n",
       "      <td>waiting the college semester to close just to make sure I will not need to reconfigure my environment again __eou__</td>\n",
       "      <td>I was calling myself a jerk. All I know is that you downloaded a game successfully. __eou__</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Context  \\\n",
       "0  anyone knows why my stock oneiric exports env var 'USERNAME'?  I mean what is that used for?  I know of $USER but not $USERNAME .  My precise install doesn't export USERNAME __eou__ __eot__ looks like it used to be exported by lightdm, but the line had the comment \"// FIXME: Is this required?\" so I guess it isn't surprising it is gone __eou__ __eot__ thanks!  How the heck did you figure that out? __eou__ __eot__ https://bugs.launchpad.net/lightdm/+bug/864109/comments/3 __eou__ __eot__                                                                                         \n",
       "1  i set up my hd such that i have to type a passphrase to access it at boot. how can i remove that passwrd, and just boot up normal. i did this at install, it works fine, just tired of having reboots where i need to be at terminal to type passwd in. help? __eou__ __eot__ backup your data, and re-install without encryption \"might\" be the easiest method __eou__ __eot__                                                                                                                                                                                                                   \n",
       "2  im trying to use ubuntu on my macbook pro retina __eou__ i read in the forums that ubuntu has a apple version now? __eou__ __eot__  not that ive ever heard of..  normal ubutnu should work on an intel based mac. there is the PPC version also. __eou__  you want total control? or what are you wanting exactly? __eou__ __eot__                                                                                                                                                                                                                                                               \n",
       "3  no suggestions? __eou__ links? __eou__ how can i remove luks passphrase at boot. i dont want to use feature anymore... __eou__ __eot__ you may need to create a new volume __eou__ __eot__ that leads me to the next question lol... i dont know how to create new volumes exactly in cmdline, usually i use a gui. im just trying to access this server via usb loaded with next os im going to load, the luks pw is stopping me __eou__ __eot__ for something like that I would likely use something like a live gparted disk to avoid the conflict of editing from the disk __eou__ __eot__    \n",
       "4  I just added a second usb printer but not sure what the uri should read - can anyone help with usb printers? __eou__ __eot__ firefox localhost:631 __eou__ __eot__ firefox? __eou__ __eot__ yes __eou__ firefox localhost:631 __eou__ firefox http://localhost:631 __eou__ cups has a web based interface __eou__ __eot__                                                                                                                                                                                                                                                                         \n",
       "\n",
       "                                                                                                             Ground Truth Utterance  \\\n",
       "0  nice thanks! __eou__                                                                                                               \n",
       "1  so you dont know, ok, anyone else? __eou__ you are like, yah my mouse doesnt work, reinstall your os lolol what a joke __eou__     \n",
       "2  just wondering how it runs __eou__                                                                                                 \n",
       "3  you cant load anything via usb or cd when luks is running __eou__ it wont allow usb boot, i tried with 2 diff usb drives __eou__   \n",
       "4  i was setting it up under the printer configuration __eou__ thanks! __eou__                                                        \n",
       "\n",
       "                                                                                                                                                                Distractor_0  \\\n",
       "0  wrong channel for it, but check efnet.org, unofficial page. __eou__                                                                                                         \n",
       "1  nmap is nice, but it wasn't what I was looking for.  I finally found it again: mtr (my traceroute) is what I was looking for.  I'll be keeping nmap handy though. __eou__   \n",
       "2  yes, that's what I did, exported it to a \"id_dsa\" file, then back to Ubuntu copied it into ~/.ssh/ __eou__                                                                  \n",
       "3    -p  sorry... __eou__  nmap -p22 __eou__ It doesn't say:  22/tcp open  ssh  ? __eou__                                                                                      \n",
       "4  i'd say the most commonly venue would be via Launchpad. check out the factoid !bug as well __eou__                                                                          \n",
       "\n",
       "                                                                                                                                                                   Distractor_1  \\\n",
       "0  every time the kernel changes, you will lose video __eou__ yep __eou__                                                                                                         \n",
       "1  ok __eou__                                                                                                                                                                     \n",
       "2  nothing - i am talking about the question of myhero __eou__                                                                                                                    \n",
       "3  i guess so i can't even launch it. __eou__                                                                                                                                     \n",
       "4  the old hardy man page, http://manpages.ubuntu.com/manpages/hardy/man1/gcalctool.1.html says \"delete\" clears the screen, but it doesn't __eou__ because LTS are good __eou__   \n",
       "\n",
       "                                                                                                                                                     Distractor_2  \\\n",
       "0  ok __eou__                                                                                                                                                       \n",
       "1   cdrom worked fine on windows. __eou__ i dont think it has anything to do with the buring process, cds work fine on my desktop and my other ubuntu lap __eou__   \n",
       "2  that should fix the fonts being too large __eou__                                                                                                                \n",
       "3  noted __eou__                                                                                                                                                    \n",
       "4  i'll give a try __eou__                                                                                                                                          \n",
       "\n",
       "                                                                                                                                                                                        Distractor_3  \\\n",
       "0  !nomodeset > acer __eou__ I'm assuming it is a driver issue. __eou__ !pm > acer __eou__ i DON'T pm. ;) __eou__ OOPS SORRY FOR THE CAPS __eou__                                                      \n",
       "1  ah yes, i have read return as rerun __eou__                                                                                                                                                         \n",
       "2  okay, so hcitool echos back hci0 <mac address of controller> but the bluetooth devices panel keeps disconnecting and reconnecting the device (or so it seems) any idea why that would be? __eou__   \n",
       "3  rxvt-unicode is one __eou__                                                                                                                                                                         \n",
       "4  by the way, the url you posted for davfs is from dapper... that's 5.xx iirc __eou__                                                                                                                 \n",
       "\n",
       "                                                                                                                              Distractor_4  \\\n",
       "0  http://www.ubuntu.com/project/about-ubuntu/derivatives  (some call them derivatives, others call them flavors, same difference) __eou__   \n",
       "1  hm? __eou__                                                                                                                               \n",
       "2  I get to the menu with options such as 'try ubuntu', 'install ubuntu', 'check disc' __eou__                                               \n",
       "3  I tarred all of ~ __eou__                                                                                                                 \n",
       "4  http://ubuntuforums.org/showthread.php?t=1549847 __eou__                                                                                  \n",
       "\n",
       "                                                                                                                                                                                                                  Distractor_5  \\\n",
       "0  thx __eou__ unfortunately the program isn't installed from the repositories __eou__                                                                                                                                           \n",
       "1  not the case, LTS is every other .04 release. The .04 isn't always more stable __eou__ I would reinstall with Precise __eou__ you can restore user data and such from backup __eou__                                          \n",
       "2  why do u need analyzer __eou__ it is a toy __eou__ ok msp301 __eou__ but y, i mean it is the same ubunut, only with older programs __eou__ ubuntu 804 or 1204 __eou__ no i dont use 804 __eou__ i am asking hypo qs __eou__   \n",
       "3  I tarred all of ~ __eou__                                                                                                                                                                                                     \n",
       "4  So I load up putty gui, then what do I do? __eou__                                                                                                                                                                            \n",
       "\n",
       "                                                                                                                                                               Distractor_6  \\\n",
       "0  how can I check? By doing a recovery for testing? __eou__                                                                                                                  \n",
       "1  Pretty much __eou__                                                                                                                                                        \n",
       "2  Cntrl-C may stop the command but it doesn't fix my HDD problem. __eou__                                                                                                    \n",
       "3  I don't really know if I can help, but I was curious. lol __eou__ That's cool. I'll look into it. Now, we better stop talking about this since it's offtopic. :P __eou__   \n",
       "4   you should read error messages, it says 'are you root?' __eou__                                                                                                           \n",
       "\n",
       "                                                                                                                                                              Distractor_7  \\\n",
       "0  my humble apologies __eou__                                                                                                                                               \n",
       "1  I used the one I downloaded from AMD __eou__                                                                                                                              \n",
       "2  if you're only going to run Ubuntu, just get a normal PC rather than a mac __eou__ that said, I'm running it on a macbook, because I got one relatively cheaply __eou__   \n",
       "3  that works just fine, thanks! __eou__                                                                                                                                     \n",
       "4  waiting the college semester to close just to make sure I will not need to reconfigure my environment again __eou__                                                       \n",
       "\n",
       "                                                                                                                  Distractor_8  \n",
       "0  #ubuntu-offtopic __eou__                                                                                                     \n",
       "1  ffmpeg is part of the package , quixotedon , at least I'm quite sure it still is __eou__ if not just install ffmpeg __eou__  \n",
       "2  the ones which are not picked up at the moment are on STDERR and not STDOUT and > is only covering STDOUT __eou__            \n",
       "3  thank you __eou__                                                                                                            \n",
       "4  I was calling myself a jerk. All I know is that you downloaded a game successfully. __eou__                                  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наши данные состоят только из контекста, реплики и метки, является ли эта реплика продолжением контекста или нет. Её можно вопринимать как целевую переменную. Поэтому EDA, например, распределение одних фичей относительно других и их возможный вклад в целевую переменную здесь не имеет смысла. Целевая переменная бинарная, по ней тоже ничего особенного посмотришь. Вообще датасет довольно straightforward, т.е. понятно, для чего конкретно он предназначен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наши данные - примеры диалогов с довольно длинным контекстом перед целевой репликой, что позволяет учитывать больший контекст в задаче question answering (и в принципе в задаче генерации близких к естественным реплик чат-бота), тогда как в классической постановке задачи существуют только пары \"предыдущее предложение - ответная реплик\", а контекст учитывается с помощью разных dialogue state tracker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теоретически, с помощью подобного корпуса можно было бы решать следующие задачи:\n",
    "- генеративная задача предсказания следующей реплики (Next Sentence Prediction)\n",
    "- дискриминативная задача выбора лучшей реплики из нескольких (на что намекает строение test data, содержащего 10 вариантов ответа для каждого контекста)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другие NLP-задачи можно обучать на данном корпусе только с некоторой доразметкой. Например, если каждому Context сопоставить тему с помощью ручной разметки, то можно будет обучать Topic Modeling, что позволит Убунту-форуму отправлять чаты в разделы соответстующей тематики."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако практически я считаю, что этот корпус не позволит обучить достаточно хорошую модель генерации ответных реплик, потому что:\n",
    "- ответы в этом Убунту-корпусе довольно технические, требуют external knowledge\n",
    "- последняя реплика контекста - далеко не всегда вопрос, и ответные реплики могут быть непредсказуемыми, например, содержать новый вопрос с уточнением"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом задача выбора наиболее подходящего ответа из нескольких выглядит многообещающе. В реальной ситуации может быть такое, что чат-бот с помощью какой-то другой модели и/или набора шаблонов генерирует варианты ответов, а модель, обученная на Убунту-корпусе, выбирает из них лучший. Точнее, это был бы бейзлайн, потому что в данном корпусе в качестве неправильных ответов (distractor) предлагаются рандомные реплики, а для улучшения качества надо бы в качестве дистракторов предлагать реплики, к примеру, одной тематики."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В следующем разделе я реализую задачу выбора лучшей реплики из нескольких с помощью предоставленного датасета, условно назвав её Next Sentence Choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Sentence Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый важный шаг в NLP - перевод текста в эмбеддинги, и не секрет, что с недавних пор лучшими являются BERT embeddings. По счастливому совпадению, BERT обучался на двух задачах - Masked Language Modeling и Next Sentence Prediction, и вторая, с небольшими изменениями, именно то, что нам надо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому бейзлайн - написать препроцессинг наших данных, загнать его в Берт и померить качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped\n"
     ]
    }
   ],
   "source": [
    "# Скачать подходящую бертовскую модель\n",
    "bert_model_name='google-bert'\n",
    "bert_model_dir = './models/'\n",
    "bert_model_path = os.path.join(bert_model_dir, bert_model_name)\n",
    "\n",
    "if not os.path.exists(bert_model_path):\n",
    "    if not os.path.exists(bert_model_dir):\n",
    "        os.mkdir(bert_model_dir)\n",
    "    bert_url = 'https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip'\n",
    "    wget.download(bert_url, out=bert_model_path+'.zip')\n",
    "    zipdata = ZipFile(bert_model_path+'.zip')\n",
    "    zipinfos = zipdata.infolist()\n",
    "    for zipinfo in zipinfos:\n",
    "        zipinfo.filename = zipinfo.filename.replace('uncased_L-12_H-768_A-12', bert_model_name)\n",
    "        zipdata.extract(zipinfo, path=bert_model_dir)\n",
    "    print(\"Unzipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузить её в keras с помощью библиотеки keras-bert\n",
    "config_path = os.path.join(bert_model_path, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(bert_model_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(bert_model_path, 'vocab.txt')\n",
    "\n",
    "model = load_trained_model_from_checkpoint(\n",
    "        config_path,\n",
    "        checkpoint_path,\n",
    "        training=True,\n",
    "        trainable=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (TokenEmbedding [(None, 512, 768), ( 23440896    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, 512, 768)     1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, 512, 768)     0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, 512, 768)     393216      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, 512, 768)     0           Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, 512, 768)     1536        Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     2362368     Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     0           Embedding-Norm[0][0]             \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, 512, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, 512, 768)     0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, 512, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, 512, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, 512, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, 512, 768)     0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, 512, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, 512, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, 512, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, 512, 768)     0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, 512, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, 512, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "MLM-Dense (Dense)               (None, 512, 768)     590592      Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "MLM-Norm (LayerNormalization)   (None, 512, 768)     1536        MLM-Dense[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Extract (Extract)               (None, 768)          0           Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "MLM-Sim (EmbeddingSimilarity)   (None, 512, 30522)   30522       MLM-Norm[0][0]                   \n",
      "                                                                 Embedding-Token[0][1]            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Masked (InputLayer)       (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "NSP-Dense (Dense)               (None, 768)          590592      Extract[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "MLM (Masked)                    (None, 512, 30522)   0           MLM-Sim[0][0]                    \n",
      "                                                                 Input-Masked[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "NSP (Dense)                     (None, 2)            1538        NSP-Dense[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 110,106,428\n",
      "Trainable params: 110,106,428\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Архитектура модели\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'MLM/Identity:0' shape=(?, 512, 30522) dtype=float32>,\n",
       " <tf.Tensor 'NSP/Softmax:0' shape=(?, 2) dtype=float32>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Из двух выходов модели нам нужен NSP - Next Sentence Prediction\n",
    "inputs = model.inputs[:2]\n",
    "outputs = model.get_layer('NSP').output\n",
    "new_model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Способ разметки авторов датасета достаточно странный, пытаемся восстановить оригинальный текст.\n",
    "# Эта функция костыльная, но в первом приближении её хватает.\n",
    "# Конец реплики заменяем на точку, переход речи к другому персонажу - на перевод строки.\n",
    "def remove_eou_eot(sent):\n",
    "    return sent.replace(' __eou__', '.').replace(' __eot__', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lexi/test_task/py2/lib/python3.7/site-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = FullTokenizer(vocab_file='models/google-bert/vocab.txt', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предобученная бертовская модель работает только с предложениями не больше 512 WordPiece, так что увы, обрезаем.\n",
    "# Попробуем обрезать спереди. Это антиинтуитивно, зато реплика непосредственно перед ответом сохранится полностью.\n",
    "def truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop(0)\n",
    "        else:\n",
    "            tokens_b.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Превращаем в бертовский формат: контекст и предполагаемый ответ слепляем в одно предложение через SEP\n",
    "# и кодируем индексами из бертовского словаря\n",
    "def make_pairs(sent_a, sent_b, tokenizer, seq_len=512):\n",
    "    tokenized_a = tokenizer.tokenize(remove_eou_eot(sent_a))\n",
    "    tokenized_b = tokenizer.tokenize(remove_eou_eot(sent_b))\n",
    "    truncate_seq_pair(tokenized_a, tokenized_b, seq_len - 3)\n",
    "    segments = [0] * (len(tokenized_a) + 2) + [1] * (len(tokenized_b) + 1)\n",
    "    tokens = ['[CLS]'] + tokenized_a + ['[SEP]'] + tokenized_b + ['[SEP]']\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    while len(ids) < seq_len:\n",
    "        ids.append(0)\n",
    "        segments.append(0)\n",
    "    return [ids, segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 101, 2151, 4784, ...,    0,    0,    0],\n",
       "       [   0,    0,    0, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(make_pairs(val_data.iloc[0]['Context'], val_data.iloc[0]['Ground Truth Utterance'], tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём тест-сет: по десять пар, из которых первая - истинная, другие девять - ложные\n",
    "def make_test_set(tokenizer, test_len=1000):\n",
    "    ids, segments = list(), list()\n",
    "    for i in tqdm(range(test_len)):\n",
    "        tokenized = make_pairs(test_data.iloc[i]['Context'], test_data.iloc[i]['Ground Truth Utterance'], tokenizer)\n",
    "        ids += [tokenized[0]]\n",
    "        segments += [tokenized[1]]\n",
    "        for j in range(9):\n",
    "            tokenized_tmp = make_pairs(test_data.iloc[i]['Context'], test_data.iloc[i]['Distractor_'+str(j)], tokenizer)\n",
    "            ids += [tokenized_tmp[0]]\n",
    "            segments += [tokenized_tmp[1]]\n",
    "    return np.array([ids, segments])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e596359fb95c4dcd86ed84e530ad49b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_set = make_test_set(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lexi/test_task/py2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "CPU times: user 33.2 s, sys: 9.94 s, total: 43.1 s\n",
      "Wall time: 6min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predicted = new_model.predict([test_set[0], test_set[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Измеряем accuracy: поскольку ground truth по построению всегда нулевой (по индексу), считаем долю нулей.\n",
    "def acc(predicted):\n",
    "    answers = []\n",
    "    for i in range(10, len(predicted) + 1, 10):\n",
    "        answers.append(np.argmax(predicted[:i, 0]) % 10)\n",
    "    return np.count_nonzero(np.array(answers) == 0) * 10 / len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.997"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно видеть, точность с pretrained bert на данном датасете довольно высока. В реальных задачах всё-таки хорошо бы выбирать лучший не из рандомных ответов, а более-менее подходящих, например, сгенерированных по шаблонам или полученных с помощью модели, обученной на каких-то других данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но вообще модель можно дообучить, fine-tune, для берта для разных задач хватает менее 5 эпох. Я не успела его дописать, но выглядит это примерно так:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Делаем train set. Он состоит из пар \"контекст - истинное продолжение\", \"контекст - ложное продолжение\"\n",
    "def make_train_set(tokenizer, train_len=100):\n",
    "    ids, segments, y = list(), list(), list()\n",
    "    for i in tqdm(range(train_len)):\n",
    "        tokenized = make_pairs(train_data.iloc[i]['Context'], train_data.iloc[i]['Utterance'], tokenizer)\n",
    "        ids += [tokenized[0]]\n",
    "        segments += [tokenized[1]]\n",
    "        y += [[int(train_data.iloc[i]['Label']), (int(train_data.iloc[i]['Label'] + 1) % 2)]]\n",
    "    return np.array([ids, segments]), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf'''\n",
    "    margin = 1\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = RAdam(lr=LR)\n",
    "new_model.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_train_set(tokenizer)\n",
    "new_model.fit([x[0], x[1]], y, batch_size=1, epochs=1, verbose=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
